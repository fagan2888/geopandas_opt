{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "# change for different ref location for geometries csv\n",
    "filepath = '../example_geometries.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with a dataframe of length 500.\n"
     ]
    }
   ],
   "source": [
    "# set this so that you have ~2-4 partitions per worker\n",
    "# e.g. 60 machines each with 4 workers should have ~500 partitions\n",
    "nparts = 4\n",
    "head_count = 500 # set to a # if you want to just do a subset of the total for faster ops, else None\n",
    "\n",
    "# get original csv as pandas dataframe\n",
    "pdf = pd.read_csv(filepath)[['id', 'geometry']].reset_index(drop=True)\n",
    "\n",
    "# convert to geopandas dataframe\n",
    "geometries = gpd.GeoSeries(pdf['geometry'].map(lambda x: loads(x)))\n",
    "crs = {'init': 'epsg:32154'},\n",
    "gdf = gpd.GeoDataFrame(data=pdf[['id']], crs=crs, geometry=geometries)\n",
    "\n",
    "# trim if desired\n",
    "if head_count is not None:\n",
    "    gdf = gdf.head(head_count)\n",
    "print('Working with a dataframe of length {}.'.format(len(gdf)))\n",
    "\n",
    "# clean the ids column\n",
    "gdf = gdf.drop('id', axis=1)\n",
    "gdf['id'] = gdf.index\n",
    "gdf['id'] = gdf['id'].astype(int)\n",
    "\n",
    "# we need some generic column to perform the many-to-many join on\n",
    "gdf = gdf.assign(tmp_key=0)\n",
    "\n",
    "# then convert into a dask dataframe\n",
    "gdf1 = gdf.copy()\n",
    "ddf = dd.from_pandas(gdf1, name='ddf', npartitions=nparts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate distance matrix the old way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_distances(grouped_result):\n",
    "    # we just need one geometry from the left side because\n",
    "    first_row = grouped_result.iloc[0]\n",
    "    from_geom = first_row['geometry_from'] # a shapely object\n",
    "\n",
    "    # and then convert to a GeoSeries\n",
    "    to_geoms = gpd.GeoSeries(grouped_result['geometry_to'])\n",
    "\n",
    "    # get an array of distances from the GeoSeries comparison\n",
    "    distances = to_geoms.distance(from_geom)\n",
    "    return distances.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# use dask to calculate distance matrix with geopandas\n",
    "tall_list = (dd.merge(ddf, gdf, on='tmp_key', suffixes=('_from', '_to'), npartitions=nparts).drop('tmp_key', axis=1))\n",
    "distances = (tall_list.groupby('id_from').apply(calc_distances, meta=pd.Series()))\n",
    "computed = distances.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1    17777.332006\n",
       "2    35648.613342\n",
       "3    40575.208147\n",
       "4    33844.863950\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show results\n",
    "pd.Series(computed[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate distance matrix a new way\n",
    "\n",
    "Calculate polygons' centroid distances instead of perimeter distances. If we don't need to know the edge-to-edge distance between our polygons, this is superior as it gives us a spatially-representative point for each polygon *and* most importantly allows us to vectorize our distance matrix computation.\n",
    "\n",
    "Use a vectorized euclidean distance calculator (euclidean works fine if geometries are projected, as they are in your example data -- if they're *not* projected, use a vectorized great circle distance calculator like I wrote for OSMnx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf2 = gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 148 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# convert polygons into xy centroids\n",
    "centroids = gdf2.centroid\n",
    "gdf2['x'] = centroids.map(lambda coords: coords.x)\n",
    "gdf2['y'] = centroids.map(lambda coords: coords.y)\n",
    "gdf2.drop('geometry', axis='columns', inplace=True) # makes merge faster and more memory efficient\n",
    "\n",
    "# create OD pairs by a many-to-many merge and index by from/to keys\n",
    "gdf_od = pd.merge(gdf2, gdf2, on='tmp_key', suffixes=('_from', '_to')).drop('tmp_key', axis=1)\n",
    "gdf_od = gdf_od.set_index(['id_from', 'id_to'])\n",
    "\n",
    "# calculate euclidean distance matrix, vectorized\n",
    "x1 = gdf_od['x_from']\n",
    "x2 = gdf_od['x_to']\n",
    "y1 = gdf_od['x_from']\n",
    "y2 = gdf_od['x_to']\n",
    "dist_matrix = np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "250000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id_from  id_to\n",
       "0        0            0.000000\n",
       "         1        15616.178292\n",
       "         2        47755.558683\n",
       "         3        47299.455346\n",
       "         4        47182.117362\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(head_count ** 2)\n",
    "print(len(dist_matrix))\n",
    "dist_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "This dropped our compute time from ~24 seconds to ~71 ms. And we've got a nice multiindex for storing this distance matrix efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Daskify the new technique\n",
    "\n",
    "No longer is this an embarassingly parallel problem: given part 3 above, we can vectorize the calculation. At this point, the only reason to use dask or other big data solutions is because the data can't fit in memory, not because we need to divide it up among many workers. If we can fit the data in memory, it is almost surely faster to do this like part 3 above than experience the dask overhead. Also, dask doesn't support multiindexing, so we can't do the nice index like in part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf3 = gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# convert polygons into xy centroids\n",
    "centroids = gdf3.centroid\n",
    "gdf3['x'] = centroids.map(lambda coords: coords.x)\n",
    "gdf3['y'] = centroids.map(lambda coords: coords.y)\n",
    "gdf3.drop('geometry', axis='columns', inplace=True) # makes merge faster and more memory efficient\n",
    "\n",
    "# create a dask dataframe of OD pairs\n",
    "ddf = dd.from_pandas(gdf3, name='ddf', npartitions=nparts)\n",
    "ddf_od = dd.merge(ddf, gdf3, on='tmp_key', suffixes=('_from', '_to'), npartitions=nparts).drop('tmp_key', axis=1)\n",
    "\n",
    "# calculate euclidean distance matrix, vectorized and with dask series\n",
    "x1 = ddf_od['x_from']\n",
    "x2 = ddf_od['x_to']\n",
    "y1 = ddf_od['x_from']\n",
    "y2 = ddf_od['x_to']\n",
    "euclidean_distances = ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "dist_matrix = euclidean_distances.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n",
      "250000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1    15616.178292\n",
       "2    47755.558683\n",
       "3    47299.455346\n",
       "4    47182.117362\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(head_count ** 2)\n",
    "print(len(dist_matrix))\n",
    "dist_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Using dask increases our compute time from ~71 ms to ~108 ms, but allows for dividing the work among multiple machines if memory usage becomes the bottleneck. But, we don't have that nice multiindex for quick pairwise lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
